{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce9b12da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers\n",
    "from sklearn.metrics import (confusion_matrix, precision_score, auc,\n",
    "                             roc_curve, recall_score, classification_report, f1_score)\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "import scipy.stats as stats\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import csv\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn_som.som import SOM\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f784dda",
   "metadata": {},
   "source": [
    "# Dados NSL-KDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe194606",
   "metadata": {},
   "outputs": [],
   "source": [
    "kddtrain = pd.read_csv(\"kddtrain_onehot.csv\").to_numpy()\n",
    "kddtest = pd.read_csv(\"kddtest_onehot.csv\").to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d15fc3",
   "metadata": {},
   "source": [
    "# Abordagem Autoencoder-KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e34590a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A944B58A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A944B58A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "136/136 [==============================] - 1s 1ms/step - loss: 0.1036\n",
      "Epoch 2/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0734\n",
      "Epoch 3/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0529\n",
      "Epoch 4/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0463\n",
      "Epoch 5/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0436\n",
      "Epoch 6/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0417\n",
      "Epoch 7/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0401\n",
      "Epoch 8/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0384\n",
      "Epoch 9/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0371\n",
      "Epoch 10/50\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0348\n",
      "Epoch 11/50\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0234\n",
      "Epoch 12/50\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0145\n",
      "Epoch 13/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0072\n",
      "Epoch 14/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0065\n",
      "Epoch 15/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0061\n",
      "Epoch 16/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0058\n",
      "Epoch 17/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0056\n",
      "Epoch 18/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0055\n",
      "Epoch 19/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0053\n",
      "Epoch 20/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0052\n",
      "Epoch 21/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0051\n",
      "Epoch 22/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0050\n",
      "Epoch 23/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0049\n",
      "Epoch 24/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0048\n",
      "Epoch 25/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0047\n",
      "Epoch 26/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0046\n",
      "Epoch 27/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0045\n",
      "Epoch 28/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0045\n",
      "Epoch 29/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0044\n",
      "Epoch 30/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0044\n",
      "Epoch 31/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0043\n",
      "Epoch 32/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0043\n",
      "Epoch 33/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0042\n",
      "Epoch 34/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0041\n",
      "Epoch 35/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0041\n",
      "Epoch 36/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0040\n",
      "Epoch 37/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0040\n",
      "Epoch 38/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0040\n",
      "Epoch 39/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0039\n",
      "Epoch 40/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0039\n",
      "Epoch 41/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0039\n",
      "Epoch 42/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0039\n",
      "Epoch 43/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 44/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 45/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 46/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 47/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 48/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0037\n",
      "Epoch 49/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0037\n",
      "Epoch 50/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0037\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A9456B8948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A9456B8948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A9456B8798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A9456B8798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A944B58828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A944B58828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "136/136 [==============================] - 5s 2ms/step - loss: 0.0447\n",
      "Epoch 2/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0137\n",
      "Epoch 3/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0107\n",
      "Epoch 4/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0063\n",
      "Epoch 6/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0051\n",
      "Epoch 7/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0043\n",
      "Epoch 8/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0039\n",
      "Epoch 9/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0036\n",
      "Epoch 10/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 11/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0032\n",
      "Epoch 12/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 13/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 14/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 15/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 16/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 17/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 18/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 19/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 20/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 21/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 22/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0021\n",
      "Epoch 23/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0021\n",
      "Epoch 24/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0020\n",
      "Epoch 25/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0020\n",
      "Epoch 26/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0020\n",
      "Epoch 27/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0019\n",
      "Epoch 28/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0019\n",
      "Epoch 29/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0019\n",
      "Epoch 30/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0019\n",
      "Epoch 31/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0018\n",
      "Epoch 32/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0018\n",
      "Epoch 33/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0018\n",
      "Epoch 34/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0018\n",
      "Epoch 35/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0018\n",
      "Epoch 36/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0018\n",
      "Epoch 37/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0018\n",
      "Epoch 38/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0018\n",
      "Epoch 39/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0017\n",
      "Epoch 40/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0017\n",
      "Epoch 41/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0017\n",
      "Epoch 42/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0017\n",
      "Epoch 43/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0017\n",
      "Epoch 44/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0017\n",
      "Epoch 45/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0017\n",
      "Epoch 46/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0017\n",
      "Epoch 47/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0017\n",
      "Epoch 48/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0017\n",
      "Epoch 49/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0017\n",
      "Epoch 50/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0017\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A9453ED5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A9453ED5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A9454ADA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A9454ADA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A94556C828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A94556C828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "136/136 [==============================] - 4s 2ms/step - loss: 0.0690\n",
      "Epoch 2/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0327\n",
      "Epoch 3/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0286\n",
      "Epoch 4/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0269\n",
      "Epoch 5/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0260\n",
      "Epoch 6/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0230\n",
      "Epoch 7/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0213\n",
      "Epoch 8/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0199\n",
      "Epoch 9/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0193\n",
      "Epoch 10/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0189\n",
      "Epoch 11/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0186\n",
      "Epoch 12/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0184\n",
      "Epoch 13/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0182\n",
      "Epoch 14/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0180\n",
      "Epoch 15/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0179\n",
      "Epoch 16/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0178\n",
      "Epoch 17/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0177\n",
      "Epoch 18/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0176\n",
      "Epoch 19/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0175\n",
      "Epoch 20/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0175\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0174\n",
      "Epoch 22/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0174\n",
      "Epoch 23/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0173\n",
      "Epoch 24/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0173\n",
      "Epoch 25/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0172\n",
      "Epoch 26/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0172\n",
      "Epoch 27/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0172\n",
      "Epoch 28/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0171\n",
      "Epoch 29/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0171\n",
      "Epoch 30/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0171\n",
      "Epoch 31/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0171\n",
      "Epoch 32/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0170\n",
      "Epoch 33/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0170\n",
      "Epoch 34/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0170\n",
      "Epoch 35/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0170\n",
      "Epoch 36/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0169\n",
      "Epoch 37/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0169\n",
      "Epoch 38/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0169\n",
      "Epoch 39/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0169\n",
      "Epoch 40/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0169\n",
      "Epoch 41/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0168\n",
      "Epoch 42/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0168\n",
      "Epoch 43/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0168\n",
      "Epoch 44/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0168\n",
      "Epoch 45/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0168\n",
      "Epoch 46/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0167\n",
      "Epoch 47/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0167\n",
      "Epoch 48/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0167\n",
      "Epoch 49/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0167\n",
      "Epoch 50/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0167\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A9497A89D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A9497A89D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A949BF7F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A949BF7F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A949FCDEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A949FCDEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "136/136 [==============================] - 3s 2ms/step - loss: 0.0525\n",
      "Epoch 2/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0238\n",
      "Epoch 3/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0195\n",
      "Epoch 4/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0177\n",
      "Epoch 5/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0166\n",
      "Epoch 6/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0158\n",
      "Epoch 7/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0151\n",
      "Epoch 8/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0146\n",
      "Epoch 9/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0142\n",
      "Epoch 10/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0140\n",
      "Epoch 11/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0138\n",
      "Epoch 12/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0137\n",
      "Epoch 13/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0136\n",
      "Epoch 14/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0135\n",
      "Epoch 15/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0134\n",
      "Epoch 16/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0133\n",
      "Epoch 17/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0133\n",
      "Epoch 18/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0132\n",
      "Epoch 19/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0132\n",
      "Epoch 20/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0131\n",
      "Epoch 21/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0131\n",
      "Epoch 22/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0131\n",
      "Epoch 23/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0130\n",
      "Epoch 24/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0130\n",
      "Epoch 25/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0130\n",
      "Epoch 26/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0129\n",
      "Epoch 27/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0129\n",
      "Epoch 28/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0129\n",
      "Epoch 29/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0129\n",
      "Epoch 30/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0128\n",
      "Epoch 31/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0128\n",
      "Epoch 32/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0127\n",
      "Epoch 33/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0127\n",
      "Epoch 34/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0127\n",
      "Epoch 35/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0126\n",
      "Epoch 36/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0126\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0126\n",
      "Epoch 38/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0125\n",
      "Epoch 39/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0125\n",
      "Epoch 40/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0125\n",
      "Epoch 41/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0125\n",
      "Epoch 42/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0125\n",
      "Epoch 43/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0124\n",
      "Epoch 44/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0124\n",
      "Epoch 45/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0124\n",
      "Epoch 46/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0124\n",
      "Epoch 47/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0124\n",
      "Epoch 48/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0124\n",
      "Epoch 49/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0124\n",
      "Epoch 50/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0124\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A94828A558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A94828A558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A94828AA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A94828AA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A9488641F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A9488641F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "136/136 [==============================] - 3s 2ms/step - loss: 0.0654\n",
      "Epoch 2/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0438\n",
      "Epoch 3/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0412\n",
      "Epoch 4/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0398\n",
      "Epoch 5/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0389\n",
      "Epoch 6/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0382\n",
      "Epoch 7/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0376\n",
      "Epoch 8/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0371\n",
      "Epoch 9/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0367\n",
      "Epoch 10/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0365\n",
      "Epoch 11/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0363\n",
      "Epoch 12/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0362\n",
      "Epoch 13/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0360\n",
      "Epoch 14/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0359\n",
      "Epoch 15/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0358\n",
      "Epoch 16/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0357\n",
      "Epoch 17/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0356\n",
      "Epoch 18/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0356\n",
      "Epoch 19/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0355\n",
      "Epoch 20/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0354\n",
      "Epoch 21/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0354\n",
      "Epoch 22/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0354\n",
      "Epoch 23/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0353\n",
      "Epoch 24/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0353\n",
      "Epoch 25/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0353\n",
      "Epoch 26/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0353\n",
      "Epoch 27/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0352\n",
      "Epoch 28/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0352\n",
      "Epoch 29/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0352\n",
      "Epoch 30/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0352\n",
      "Epoch 31/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0352\n",
      "Epoch 32/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0352\n",
      "Epoch 33/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0351\n",
      "Epoch 34/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0351\n",
      "Epoch 35/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0351\n",
      "Epoch 36/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0351\n",
      "Epoch 37/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0351\n",
      "Epoch 38/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0351\n",
      "Epoch 39/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0351\n",
      "Epoch 40/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0351\n",
      "Epoch 41/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0351\n",
      "Epoch 42/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0351\n",
      "Epoch 43/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0350\n",
      "Epoch 44/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0350\n",
      "Epoch 45/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0350\n",
      "Epoch 46/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0350\n",
      "Epoch 47/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0350\n",
      "Epoch 48/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0350\n",
      "Epoch 49/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0350\n",
      "Epoch 50/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0350\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A94A64D318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A94A64D318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A9456B8D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A9456B8D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A948571948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A948571948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "136/136 [==============================] - 3s 2ms/step - loss: 0.0645\n",
      "Epoch 2/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0457\n",
      "Epoch 3/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0312\n",
      "Epoch 4/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0274\n",
      "Epoch 5/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0258\n",
      "Epoch 6/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0246\n",
      "Epoch 7/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0239\n",
      "Epoch 8/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0233\n",
      "Epoch 9/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0230\n",
      "Epoch 10/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0227\n",
      "Epoch 11/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0225\n",
      "Epoch 12/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0223\n",
      "Epoch 13/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0222\n",
      "Epoch 14/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0221A: 0s - loss: 0\n",
      "Epoch 15/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0220\n",
      "Epoch 16/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0219\n",
      "Epoch 17/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0218\n",
      "Epoch 18/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0218\n",
      "Epoch 19/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0217\n",
      "Epoch 20/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0217\n",
      "Epoch 21/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0216\n",
      "Epoch 22/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0216\n",
      "Epoch 23/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0216\n",
      "Epoch 24/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0215\n",
      "Epoch 25/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0215\n",
      "Epoch 26/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0215\n",
      "Epoch 27/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0214\n",
      "Epoch 28/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0214\n",
      "Epoch 29/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0214\n",
      "Epoch 30/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0213\n",
      "Epoch 31/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0213\n",
      "Epoch 32/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0213\n",
      "Epoch 33/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0213A: 0s - loss: 0.02\n",
      "Epoch 34/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0213\n",
      "Epoch 35/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0212\n",
      "Epoch 36/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0212\n",
      "Epoch 37/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0212\n",
      "Epoch 38/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0212\n",
      "Epoch 39/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0212\n",
      "Epoch 40/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0211\n",
      "Epoch 41/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0211\n",
      "Epoch 42/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0211\n",
      "Epoch 43/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0211\n",
      "Epoch 44/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0211\n",
      "Epoch 45/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0211\n",
      "Epoch 46/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0211\n",
      "Epoch 47/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0211\n",
      "Epoch 48/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0210\n",
      "Epoch 49/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0210\n",
      "Epoch 50/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0210\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A944F51318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A944F51318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A94564D168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A94564D168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A94550E678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A94550E678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "136/136 [==============================] - 3s 2ms/step - loss: 0.0633\n",
      "Epoch 2/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0420\n",
      "Epoch 3/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0383\n",
      "Epoch 4/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0360\n",
      "Epoch 5/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0347\n",
      "Epoch 6/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0340\n",
      "Epoch 7/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0335\n",
      "Epoch 8/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0330\n",
      "Epoch 9/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0326\n",
      "Epoch 10/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0324\n",
      "Epoch 11/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0323\n",
      "Epoch 12/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0322\n",
      "Epoch 13/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0321\n",
      "Epoch 14/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0320\n",
      "Epoch 15/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0320\n",
      "Epoch 16/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0319\n",
      "Epoch 17/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0318\n",
      "Epoch 18/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0318\n",
      "Epoch 19/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0317\n",
      "Epoch 20/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0317\n",
      "Epoch 21/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0316\n",
      "Epoch 22/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0316\n",
      "Epoch 23/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0315\n",
      "Epoch 24/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0315\n",
      "Epoch 25/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0315\n",
      "Epoch 26/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0314\n",
      "Epoch 27/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0314\n",
      "Epoch 28/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0314\n",
      "Epoch 29/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0314\n",
      "Epoch 30/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0314\n",
      "Epoch 31/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0313\n",
      "Epoch 32/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0313\n",
      "Epoch 33/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0313\n",
      "Epoch 34/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0313\n",
      "Epoch 35/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0313\n",
      "Epoch 36/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0313\n",
      "Epoch 37/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0313\n",
      "Epoch 38/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0313\n",
      "Epoch 39/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0312\n",
      "Epoch 40/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0312\n",
      "Epoch 41/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0312\n",
      "Epoch 42/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0312\n",
      "Epoch 43/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0312\n",
      "Epoch 44/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0312\n",
      "Epoch 45/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0312\n",
      "Epoch 46/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0312\n",
      "Epoch 47/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0312\n",
      "Epoch 48/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0312\n",
      "Epoch 49/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0312\n",
      "Epoch 50/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0312\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A944F96E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A944F96E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A944F96948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A944F96948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A9456B8708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A9456B8708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "136/136 [==============================] - 3s 2ms/step - loss: 0.0569\n",
      "Epoch 2/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0296\n",
      "Epoch 3/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0204\n",
      "Epoch 4/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0094\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0068\n",
      "Epoch 6/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0054\n",
      "Epoch 7/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0047\n",
      "Epoch 8/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0043\n",
      "Epoch 9/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0040\n",
      "Epoch 10/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 11/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0037\n",
      "Epoch 12/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0036\n",
      "Epoch 13/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0035\n",
      "Epoch 14/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 15/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 16/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0032\n",
      "Epoch 17/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 18/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 19/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 20/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 21/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0029\n",
      "Epoch 22/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0028\n",
      "Epoch 23/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 24/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0027\n",
      "Epoch 25/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 26/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 27/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 28/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 29/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 30/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0026\n",
      "Epoch 31/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 32/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0026\n",
      "Epoch 33/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 34/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0025\n",
      "Epoch 35/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0025\n",
      "Epoch 36/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 37/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 38/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 39/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0025\n",
      "Epoch 40/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 41/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 42/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 43/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 44/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 45/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 46/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0024\n",
      "Epoch 47/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0024\n",
      "Epoch 48/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 49/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 50/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A949CFFF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A949CFFF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A949D31EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A949D31EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A94A64D1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A94A64D1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "136/136 [==============================] - 3s 2ms/step - loss: 0.0899\n",
      "Epoch 2/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0668\n",
      "Epoch 3/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0648\n",
      "Epoch 4/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0628\n",
      "Epoch 5/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0529\n",
      "Epoch 6/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0459\n",
      "Epoch 7/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0430\n",
      "Epoch 8/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0303\n",
      "Epoch 9/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0293\n",
      "Epoch 10/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0290\n",
      "Epoch 11/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0289\n",
      "Epoch 12/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0287\n",
      "Epoch 13/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0286\n",
      "Epoch 14/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0285\n",
      "Epoch 15/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0284\n",
      "Epoch 16/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0283\n",
      "Epoch 17/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0282\n",
      "Epoch 18/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0281\n",
      "Epoch 19/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0280\n",
      "Epoch 20/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0280\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0279\n",
      "Epoch 22/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0279\n",
      "Epoch 23/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0278\n",
      "Epoch 24/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0278\n",
      "Epoch 25/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0278\n",
      "Epoch 26/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0277\n",
      "Epoch 27/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0277\n",
      "Epoch 28/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0277\n",
      "Epoch 29/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0276\n",
      "Epoch 30/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0276\n",
      "Epoch 31/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0276\n",
      "Epoch 32/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0276\n",
      "Epoch 33/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0276\n",
      "Epoch 34/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0276\n",
      "Epoch 35/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0275\n",
      "Epoch 36/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0275\n",
      "Epoch 37/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0275\n",
      "Epoch 38/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0275\n",
      "Epoch 39/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0275\n",
      "Epoch 40/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0275\n",
      "Epoch 41/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0275\n",
      "Epoch 42/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0275\n",
      "Epoch 43/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0275\n",
      "Epoch 44/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0275\n",
      "Epoch 45/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0274\n",
      "Epoch 46/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0274\n",
      "Epoch 47/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0274\n",
      "Epoch 48/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0274\n",
      "Epoch 49/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0274\n",
      "Epoch 50/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0274\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A9456A0438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A9456A0438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A9487F2C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A9487F2C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A949DB5948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A949DB5948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "136/136 [==============================] - 3s 2ms/step - loss: 0.0528\n",
      "Epoch 2/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0145\n",
      "Epoch 3/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0103\n",
      "Epoch 4/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0086\n",
      "Epoch 5/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 6/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0065\n",
      "Epoch 7/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0058\n",
      "Epoch 8/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0054\n",
      "Epoch 9/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0051\n",
      "Epoch 10/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0048\n",
      "Epoch 11/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0046\n",
      "Epoch 12/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0040\n",
      "Epoch 13/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0037\n",
      "Epoch 14/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0036\n",
      "Epoch 15/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0035\n",
      "Epoch 16/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 17/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 18/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 19/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0032\n",
      "Epoch 20/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 21/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 22/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 23/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 24/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 25/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 26/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 27/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 28/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0027\n",
      "Epoch 29/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0027\n",
      "Epoch 30/50\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0027\n",
      "Epoch 31/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 32/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 33/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 34/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 35/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 36/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 38/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 39/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 40/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 41/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 42/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 43/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 44/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 45/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 46/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 47/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 48/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 49/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 50/50\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A9486771F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A9486771F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A9477B2168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001A9477B2168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "data = np.concatenate((kddtrain,kddtest), axis=0)\n",
    "epochs = 50\n",
    "batch_size = 256\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "activation = \"relu\"\n",
    "neighbors = 11\n",
    "\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "num_folds = 10\n",
    "\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "fold_no = 1\n",
    "count = 0\n",
    "\n",
    "#Mtricas ErroRecons\n",
    "matrix_ErroRecons = np.zeros((2,2))\n",
    "accuracy_ErroRecons = np.zeros(num_folds)\n",
    "recall_ErroRecons = np.zeros(num_folds)\n",
    "precision_ErroRecons = np.zeros(num_folds)\n",
    "f1_ErroRecons = np.zeros(num_folds)\n",
    "\n",
    "#Mtricas KNN\n",
    "matrix_knn = np.zeros((2,2))\n",
    "accuracy_knn = np.zeros(num_folds)\n",
    "recall_knn = np.zeros(num_folds)\n",
    "precision_knn = np.zeros(num_folds)\n",
    "f1_knn = np.zeros(num_folds)\n",
    "\n",
    "#Mtricas KMeans\n",
    "matrix_kmeans = np.zeros((2,2))\n",
    "accuracy_kmeans = np.zeros(num_folds)\n",
    "recall_kmeans = np.zeros(num_folds)\n",
    "precision_kmeans = np.zeros(num_folds)\n",
    "f1_kmeans = np.zeros(num_folds)\n",
    "\n",
    "#Mtricas SOM\n",
    "matrix_som = np.zeros((2,2))\n",
    "accuracy_som = np.zeros(num_folds)\n",
    "recall_som = np.zeros(num_folds)\n",
    "precision_som = np.zeros(num_folds)\n",
    "f1_som = np.zeros(num_folds)\n",
    "\n",
    "#Mtricas SVM\n",
    "matrix_svm = np.zeros((2,2))\n",
    "accuracy_svm = np.zeros(num_folds)\n",
    "recall_svm = np.zeros(num_folds)\n",
    "precision_svm = np.zeros(num_folds)\n",
    "f1_svm = np.zeros(num_folds)\n",
    "\n",
    "for train, test in kfold.split(data[::, 0:data.shape[1]-3], data[::, data.shape[1]-1]):\n",
    "    \n",
    "    #####################################################\n",
    "    ####Pr-Processamento\n",
    "    \n",
    "    #Separao dos Folds de Treinamento e Teste\n",
    "    dataTrain = data[train]\n",
    "    dataTest = data[test]\n",
    "    \n",
    "    #Conta Quantidade de dados\n",
    "    Train = dataTrain.shape[1]\n",
    "    noTrain = np.count_nonzero(dataTrain[:,Train-1] == 1)\n",
    "    anTrain = np.count_nonzero(dataTrain[:, Train-1] == 0)\n",
    "\n",
    "    #Separa dados normais e anmalos\n",
    "    normalTrain = dataTrain[np.where(dataTrain[:,Train-1] == 1)]\n",
    "    anomalyTrain = dataTrain[np.where(dataTrain[:,Train-1] == 0)]\n",
    "    \n",
    "    #Divide dados para treinamento\n",
    "    porcen = 0.50\n",
    "    j = int(noTrain * porcen)\n",
    "    \n",
    "    #ndices escolhidos aleatoriamente\n",
    "    numbers_knn_normal = np.array(random.sample(range(0, normalTrain.shape[0]),j))\n",
    "    numbers_knn_anomaly = np.array(random.sample(range(0,anomalyTrain.shape[0]),j))\n",
    "    numbers_autoencoder_normal = np.array(list(set(np.arange(0, normalTrain.shape[0])) - set(numbers_knn_normal)))\n",
    "    \n",
    "    #Dados KNN\n",
    "    knnTrain_normal = normalTrain[numbers_knn_normal]\n",
    "    knnTrain_anomaly = anomalyTrain[numbers_knn_anomaly]\n",
    "    knnTrain = np.concatenate((knnTrain_normal, knnTrain_anomaly), axis=0)\n",
    "    \n",
    "    #Dados Autoencoder\n",
    "    autoencoderTrain = normalTrain[numbers_autoencoder_normal]\n",
    "    \n",
    "    #Normalizao\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(autoencoderTrain)\n",
    "    autoencoderTrain = scaler.transform(autoencoderTrain)\n",
    "\n",
    "    scaler1 = MinMaxScaler()\n",
    "    scaler1.fit(knnTrain)\n",
    "    knnTrain = scaler1.transform(knnTrain)\n",
    "\n",
    "    dataTest = scaler1.transform(dataTest)\n",
    "    \n",
    "    ###Separao das Caractersticas e classes\n",
    "    autoencoderTrain_class = autoencoderTrain[::, autoencoderTrain.shape[1]-2:autoencoderTrain.shape[1]]\n",
    "    autoencoderTrain = autoencoderTrain[::, 0:autoencoderTrain.shape[1]-3]\n",
    "\n",
    "    knnTrain_class = knnTrain[::, knnTrain.shape[1]-2:knnTrain.shape[1]]\n",
    "    knnTrain = knnTrain[::, 0:knnTrain.shape[1]-3]\n",
    "\n",
    "    modelTest_class = dataTest[::, dataTest.shape[1]-2:dataTest.shape[1]]\n",
    "    modelTest = dataTest[::, 0:dataTest.shape[1]-3]\n",
    "    \n",
    "    #####################################################\n",
    "    ####Autoencoder\n",
    "    #Modela as camadas do Autoencoder\n",
    "    var = autoencoderTrain.shape[1]\n",
    "    input_vector = keras.Input(shape=(var,))\n",
    "    en1 = layers.Dense(32, activation=activation)(input_vector)\n",
    "    en2 = layers.Dense(16, activation=activation)(en1)\n",
    "    de1 = layers.Dense(32, activation=activation)(en2)\n",
    "    de2 = layers.Dense(var, activation=activation)(de1)\n",
    "\n",
    "    #Gera o modelo\n",
    "    autoencoder = keras.Model(input_vector, de2)\n",
    "    \n",
    "    encoder = keras.Model(input_vector, en2)\n",
    "    encoded_input = keras.Input(shape=(16,))\n",
    "    decoder_layer = autoencoder.layers[-2]\n",
    "    decoder = keras.Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "    #Compila o modelo\n",
    "    autoencoder.compile(optimizer=optimizer, loss=\"mean_squared_error\")\n",
    "\n",
    "    #Treina o modelo\n",
    "    history = autoencoder.fit(autoencoderTrain, autoencoderTrain, epochs = epochs, batch_size = batch_size, shuffle = True)\n",
    "    \n",
    "    \n",
    "    #####################################################\n",
    "    ####Erro de Reconstruo\n",
    "    #Teste\n",
    "    pred_ErroRecons = autoencoder.predict(modelTest)\n",
    "    \n",
    "    #Extrao do erro\n",
    "    mse_ErroRecons = np.mean(np.power(modelTest - pred_ErroRecons, 2), axis=1)\n",
    "    error_df_ErroRecons = pd.DataFrame({'reconstruction_error': mse_ErroRecons,\n",
    "                              'true_class': modelTest_class[:,1]})\n",
    "    \n",
    "    #Limiar de Separao\n",
    "    media = np.mean(error_df_ErroRecons['reconstruction_error'])\n",
    "    desv = np.std(error_df_ErroRecons['reconstruction_error'])\n",
    "    Z = 0\n",
    "    teta = media + Z*desv\n",
    "    \n",
    "    #Separao dos erros\n",
    "    predi_ErroRecons = np.zeros(modelTest.shape[0])\n",
    "    error_ErroRecons = error_df_ErroRecons.to_numpy()\n",
    "    predi_ErroRecons[np.where(error_ErroRecons[:,0] <= teta)] = 1\n",
    "    \n",
    "    #Mtricas\n",
    "    ab_ErroRecons = confusion_matrix(error_ErroRecons[:,1], predi_ErroRecons[:])\n",
    "    accuracy_ErroRecons[count] = accuracy_score(error_ErroRecons[:,1], predi_ErroRecons[:])\n",
    "    recall_ErroRecons[count] = recall_score(error_ErroRecons[:,1], predi_ErroRecons[:])\n",
    "    precision_ErroRecons[count] = precision_score(error_ErroRecons[:,1], predi_ErroRecons[:])\n",
    "    f1_ErroRecons[count] = f1_score(error_ErroRecons[:,1], predi_ErroRecons[:])\n",
    "    matrix_ErroRecons = matrix_ErroRecons + ab_ErroRecons\n",
    "    \n",
    "    #####################################################\n",
    "    ####KNN\n",
    "    ###Treinamento KNN\n",
    "    pred_knn = autoencoder.predict(knnTrain)\n",
    "    mse_knn = np.mean(np.power(knnTrain - pred_knn, 2), axis=1)\n",
    "    error_df_knn = pd.DataFrame({'reconstruction_error': mse_knn,\n",
    "                                 'true_class': knnTrain_class[::,1]})\n",
    "    error_knn = error_df_knn.to_numpy()\n",
    "    encoder_layer = encoder.predict(knnTrain)\n",
    "    error_encoder = np.append(encoder_layer, error_knn, axis=1)\n",
    "    neigh = KNeighborsClassifier(n_neighbors=neighbors)\n",
    "    neigh.fit(error_encoder[:,0:error_encoder.shape[1] - 2], error_knn[:,1])\n",
    "    \n",
    "    #Teste do Modelo\n",
    "    pred_knn = autoencoder.predict(modelTest)\n",
    "    mse_knn = np.mean(np.power(modelTest - pred_knn, 2), axis=1)\n",
    "    error_df_knn = pd.DataFrame({'reconstruction_error': mse_knn,\n",
    "                                'true_class': modelTest_class[::,1]})\n",
    "    error_knn = error_df_knn.to_numpy()\n",
    "    encoder_layer = encoder.predict(modelTest)\n",
    "    error_encoder = np.append(encoder_layer, error_knn, axis=1)\n",
    "    predict_knn = neigh.predict(error_encoder[:,0:error_encoder.shape[1] - 2])\n",
    "    \n",
    "    #Mtricas\n",
    "    ab_knn = confusion_matrix(error_knn[:,1], predict_knn[:])\n",
    "    accuracy_knn[count] = accuracy_score(error_knn[:,1], predict_knn[:])\n",
    "    recall_knn[count] = recall_score(error_knn[:,1], predict_knn[:])\n",
    "    precision_knn[count] = precision_score(error_knn[:,1], predict_knn[:])\n",
    "    f1_knn[count] = f1_score(error_knn[:,1], predict_knn[:])\n",
    "    matrix_knn = matrix_knn + ab_knn\n",
    "    \n",
    "    #####################################################\n",
    "    ####KMeans\n",
    "    ###Treinamento KMeans \n",
    "    pred_kmeans = autoencoder.predict(knnTrain)\n",
    "    mse_kmeans = np.mean(np.power(knnTrain - pred_kmeans, 2), axis=1)\n",
    "    error_df_kmeans = pd.DataFrame({'reconstruction_error': mse_kmeans,\n",
    "                                     'true_class': knnTrain_class[::,1]})\n",
    "    error_kmeans = error_df_kmeans.to_numpy()\n",
    "    encoder_layer = encoder.predict(knnTrain)\n",
    "    error_encoder = np.append(encoder_layer, error_kmeans, axis=1)\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0).fit(error_encoder[:,0:error_encoder.shape[1]-2])\n",
    "    kmeans.labels_\n",
    "\n",
    "    #Teste\n",
    "    pred_kmeans = autoencoder.predict(modelTest)\n",
    "    mse_kmeans = np.mean(np.power(modelTest - pred_kmeans, 2), axis=1)\n",
    "    error_df_kmeans = pd.DataFrame({'reconstruction_error': mse_kmeans,\n",
    "                                     'true_class': modelTest_class[::,1]})\n",
    "    error_kmeans = error_df_kmeans.to_numpy()\n",
    "    encoder_layer = encoder.predict(modelTest)\n",
    "    error_encoder = np.append(encoder_layer,error_kmeans, axis=1)\n",
    "    predict_kmeans = kmeans.predict(error_encoder[:,0:error_encoder.shape[1]-2])\n",
    "\n",
    "    #Mtricas\n",
    "    ab_kmeans = confusion_matrix(error_kmeans[:,1], predict_kmeans[:])\n",
    "    accuracy_kmeans[count] = accuracy_score(error_kmeans[:,1], predict_kmeans[:])\n",
    "    recall_kmeans[count] = recall_score(error_kmeans[:,1], predict_kmeans[:])\n",
    "    precision_kmeans[count] = precision_score(error_kmeans[:,1], predict_kmeans[:])\n",
    "    f1_kmeans[count] = f1_score(error_kmeans[:,1], predict_kmeans[:])\n",
    "    matrix_kmeans = matrix_kmeans + ab_kmeans\n",
    "    \n",
    "    #####################################################\n",
    "    ####SOM\n",
    "    ###Treinamento KMeans \n",
    "    pred_som = autoencoder.predict(knnTrain)\n",
    "    mse_som = np.mean(np.power(knnTrain - pred_som, 2), axis=1)\n",
    "    error_df_som = pd.DataFrame({'reconstruction_error': mse_som,\n",
    "                                         'true_class': knnTrain_class[::,1]})\n",
    "    error_som = error_df_som.to_numpy()\n",
    "    som = SOM(m=2,n=1,dim=1)\n",
    "    som.fit(error_som[:,0].reshape(-1,1))\n",
    "\n",
    "    #Teste\n",
    "    pred_som = autoencoder.predict(modelTest)\n",
    "    mse_som = np.mean(np.power(modelTest - pred_som, 2), axis=1)\n",
    "    error_df_som = pd.DataFrame({'reconstruction_error': mse_som,\n",
    "                                         'true_class': modelTest_class[::,1]})\n",
    "    error_som = error_df_som.to_numpy()\n",
    "    predict_som = som.predict(error_som[:,0].reshape(-1,1))\n",
    "\n",
    "    #Mtricas\n",
    "    ab_som = confusion_matrix(error_som[:,1], predict_som[:])\n",
    "    accuracy_som[count] = accuracy_score(error_som[:,1], predict_som[:])\n",
    "    recall_som[count] = recall_score(error_som[:,1], predict_som[:])\n",
    "    precision_som[count] = precision_score(error_som[:,1], predict_som[:])\n",
    "    f1_som[count] = f1_score(error_som[:,1], predict_som[:])\n",
    "    matrix_som = matrix_som + ab_som\n",
    "    \n",
    "    #####################################################\n",
    "    ####SVM\n",
    "    #Treinamento SVM\n",
    "    pred_svm = autoencoder.predict(knnTrain)\n",
    "    mse_svm = np.mean(np.power(knnTrain - pred_svm, 2), axis=1)\n",
    "    error_df_svm = pd.DataFrame({'reconstruction_error': mse_svm,\n",
    "                                     'true_class': knnTrain_class[::,1]})\n",
    "    error_svm = error_df_svm.to_numpy()\n",
    "    encoder_layer = encoder.predict(knnTrain)\n",
    "    error_encoder = np.append(encoder_layer, error_svm, axis=1)\n",
    "    clf = svm.SVC(kernel='rbf')\n",
    "    clf.fit(error_encoder[:,0:error_encoder.shape[1] - 2], error_svm[:,1])\n",
    "    \n",
    "    #Teste\n",
    "    pred_svm = autoencoder.predict(modelTest)\n",
    "    mse_svm = np.mean(np.power(modelTest - pred_svm, 2), axis=1)\n",
    "    error_df_svm = pd.DataFrame({'reconstruction_error': mse_svm,\n",
    "                                     'true_class': modelTest_class[::,1]})\n",
    "    error_svm = error_df_svm.to_numpy()\n",
    "    encoder_layer = encoder.predict(modelTest)\n",
    "    error_encoder = np.append(encoder_layer,error_svm, axis=1)\n",
    "    predict_svm = clf.predict(error_encoder[:,0:error_encoder.shape[1]-2])\n",
    "    \n",
    "    #Mtricas\n",
    "    ab_svm = confusion_matrix(error_svm[:,1], predict_svm[:])\n",
    "    accuracy_svm[count] = accuracy_score(error_svm[:,1], predict_svm[:])\n",
    "    recall_svm[count] = recall_score(error_svm[:,1], predict_svm[:])\n",
    "    precision_svm[count] = precision_score(error_svm[:,1], predict_svm[:])\n",
    "    f1_svm[count] = f1_score(error_svm[:,1], predict_svm[:])\n",
    "    matrix_svm = matrix_svm + ab_svm\n",
    "\n",
    "    count = count + 1\n",
    "    fold_no = fold_no + 1\n",
    "    \n",
    "matrix_knn = matrix_knn/num_folds\n",
    "matrix_kmeans = matrix_kmeans/num_folds\n",
    "matrix_ErroRecons = matrix_ErroRecons/num_folds\n",
    "matrix_som = matrix_som/num_folds\n",
    "matrix_svm = matrix_svm/num_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a03d2cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder - Erro de Reconstruo:\n",
      " Acertos:  11697 , Erros: 3153 \n",
      " [[4751 2395]\n",
      " [ 758 6946]] \n",
      "Acurcia:  0.7876560167290988 \n",
      "Recall:  0.9013228586795679 \n",
      "Precision:  0.7534900719894099 \n",
      "F1:  0.8122752714618862\n",
      "\n",
      "Autoencoder - KNN:\n",
      " Acertos: 14551 , Erros: 300 \n",
      " [[6991  154]\n",
      " [ 146 7559]] \n",
      "Acurcia:  0.9797663306485141 \n",
      "Recall:  0.9810243062030579 \n",
      "Precision:  0.979997825928167 \n",
      "F1:  0.9805077134445419\n",
      "\n",
      "Autoencoder - SVM:\n",
      " Acertos: 14198 , Erros: 653 \n",
      " [[6670  476]\n",
      " [ 177 7527]] \n",
      "Acurcia:  0.9560044312511373 \n",
      "Recall:  0.9769795334864758 \n",
      "Precision:  0.9405607061960028 \n",
      "F1:  0.9584071631533397\n",
      "\n",
      "Autoencoder - KMeans:\n",
      " Acertos: 7380 , Erros: 7470 \n",
      " [[4314 2831]\n",
      " [4638 3066]] \n",
      "Acurcia:  0.4969731716775178 \n",
      "Recall:  0.3970803672078372 \n",
      "Precision:  0.4034381222667416 \n",
      "F1:  0.3861407541312579\n",
      "\n",
      "Autoencoder - SOM:\n",
      " Acertos: 5960 , Erros: 8890 \n",
      " [[3043 4102]\n",
      " [4788 2916]] \n",
      "Acurcia:  0.4013511206560443 \n",
      "Recall:  0.3784070757989562 \n",
      "Precision:  0.33098221107947623 \n",
      "F1:  0.3514442206419899\n"
     ]
    }
   ],
   "source": [
    "print(\"Autoencoder - Erro de Reconstruo:\\n\",\"Acertos: \", int(matrix_ErroRecons[0,0]+matrix_ErroRecons[1,1]), \", Erros:\", int(matrix_ErroRecons[0,1]+matrix_ErroRecons[1,0]) ,\"\\n\", matrix_ErroRecons.astype(int), \"\\nAcurcia: \", sum(accuracy_ErroRecons)/num_folds, \"\\nRecall: \", sum(recall_ErroRecons)/num_folds, \"\\nPrecision: \", sum(precision_ErroRecons)/num_folds, \"\\nF1: \", sum(f1_ErroRecons)/num_folds)\n",
    "\n",
    "print(\"\\nAutoencoder - KNN:\\n\",\"Acertos:\", int(matrix_knn[0,0]+matrix_knn[1,1]), \", Erros:\", int(matrix_knn[0,1]+matrix_knn[1,0]) ,\"\\n\", matrix_knn.astype(int), \"\\nAcurcia: \", sum(accuracy_knn)/num_folds, \"\\nRecall: \", sum(recall_knn)/num_folds, \"\\nPrecision: \", sum(precision_knn)/num_folds, \"\\nF1: \", sum(f1_knn)/num_folds)\n",
    "\n",
    "print(\"\\nAutoencoder - SVM:\\n\",\"Acertos:\", int(matrix_svm[0,0]+matrix_svm[1,1]), \", Erros:\", int(matrix_svm[0,1]+matrix_svm[1,0]) ,\"\\n\", matrix_svm.astype(int), \"\\nAcurcia: \", sum(accuracy_svm)/num_folds, \"\\nRecall: \", sum(recall_svm)/num_folds, \"\\nPrecision: \", sum(precision_svm)/num_folds, \"\\nF1: \", sum(f1_svm)/num_folds)\n",
    "\n",
    "print(\"\\nAutoencoder - KMeans:\\n\",\"Acertos:\", int(matrix_kmeans[0,0]+matrix_kmeans[1,1]), \", Erros:\", int(matrix_kmeans[0,1]+matrix_kmeans[1,0]) ,\"\\n\", matrix_kmeans.astype(int), \"\\nAcurcia: \", sum(accuracy_kmeans)/num_folds, \"\\nRecall: \", sum(recall_kmeans)/num_folds, \"\\nPrecision: \", sum(precision_kmeans)/num_folds, \"\\nF1: \", sum(f1_kmeans)/num_folds)\n",
    "\n",
    "print(\"\\nAutoencoder - SOM:\\n\",\"Acertos:\", int(matrix_som[0,0]+matrix_som[1,1]), \", Erros:\", int(matrix_som[0,1]+matrix_som[1,0]) ,\"\\n\", matrix_som.astype(int), \"\\nAcurcia: \", sum(accuracy_som)/num_folds, \"\\nRecall: \", sum(recall_som)/num_folds, \"\\nPrecision: \", sum(precision_som)/num_folds, \"\\nF1: \", sum(f1_som)/num_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c9c802a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Salva Resultados\n",
    "with open('Resultados_Erro+InterLayer.csv', 'a', newline='') as csvfile:\n",
    "    fieldnames = ['Algoritmo','Mtrica', 'Mdia']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    writer.writerow({'Algoritmo': 'ErroRecons','Mtrica': accuracy_ErroRecons, 'Mdia': sum(accuracy_ErroRecons)/num_folds})\n",
    "    writer.writerow({'Algoritmo': 'ErroRecons','Mtrica': recall_ErroRecons, 'Mdia': sum(recall_ErroRecons)/num_folds})\n",
    "    writer.writerow({'Algoritmo': 'ErroRecons','Mtrica': precision_ErroRecons, 'Mdia': sum(precision_ErroRecons)/num_folds})\n",
    "    writer.writerow({'Algoritmo': 'ErroRecons','Mtrica': f1_ErroRecons, 'Mdia': sum(f1_ErroRecons)/num_folds})\n",
    "    \n",
    "    \n",
    "    writer.writerow({'Algoritmo': 'KNN','Mtrica': accuracy_knn, 'Mdia': sum(accuracy_knn)/num_folds})\n",
    "    writer.writerow({'Algoritmo': 'KNN','Mtrica': recall_knn, 'Mdia': sum(recall_knn)/num_folds})\n",
    "    writer.writerow({'Algoritmo': 'KNN','Mtrica': precision_knn, 'Mdia': sum(precision_knn)/num_folds})\n",
    "    writer.writerow({'Algoritmo': 'KNN','Mtrica': f1_knn, 'Mdia': sum(f1_knn)/num_folds})\n",
    "    \n",
    "    \n",
    "    writer.writerow({'Algoritmo': 'SVM','Mtrica': accuracy_svm, 'Mdia': sum(accuracy_svm)/num_folds})\n",
    "    writer.writerow({'Algoritmo': 'SVM','Mtrica': recall_svm, 'Mdia': sum(recall_svm)/num_folds})\n",
    "    writer.writerow({'Algoritmo': 'SVM','Mtrica': precision_svm, 'Mdia': sum(precision_svm)/num_folds})\n",
    "    writer.writerow({'Algoritmo': 'SVM','Mtrica': f1_svm, 'Mdia': sum(f1_svm)/num_folds})\n",
    "    \n",
    "    writer.writerow({'Algoritmo': 'KMeans','Mtrica': accuracy_kmeans, 'Mdia': sum(accuracy_kmeans)/num_folds})\n",
    "    writer.writerow({'Algoritmo': 'KMeans','Mtrica': recall_kmeans, 'Mdia': sum(recall_kmeans)/num_folds})\n",
    "    writer.writerow({'Algoritmo': 'KMeans','Mtrica': precision_kmeans, 'Mdia': sum(precision_kmeans)/num_folds})\n",
    "    writer.writerow({'Algoritmo': 'KMeans','Mtrica': f1_kmeans, 'Mdia': sum(f1_kmeans)/num_folds})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
